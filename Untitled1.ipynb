{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNGiRB+Nkeq84E3HRthAZ6K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashdev-7/agent-eye/blob/main/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dS1X-YQkAlYN",
        "outputId": "1fed2b71-1d9e-4c19-fcf0-d202841a2c7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folders ready.\n"
          ]
        }
      ],
      "source": [
        "# Core imports\n",
        "import os, random, json, math, numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "np.random.seed(42); random.seed(42)\n",
        "\n",
        "# Keras / TF\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Create project structure\n",
        "for d in [\"data/raw\",\"data/processed/cmaps\",\"src\",\"figs\",\"tables\",\"logs\"]:\n",
        "    Path(d).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"Folders ready.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "TRAIN = \"/content/data/raw/train_FD001.txt\"   # your path\n",
        "TEST  = \"/content/data/raw/test_FD001.txt\"    # your path\n",
        "RUL   = \"/content/data/raw/RUL_FD001.txt\"     # your path\n",
        "\n",
        "# Read raw with whitespace parsing, no header\n",
        "raw_tr = pd.read_csv(TRAIN, delim_whitespace=True, header=None)\n",
        "raw_te = pd.read_csv(TEST,  delim_whitespace=True, header=None)\n",
        "print(\"Raw shapes:\", raw_tr.shape, raw_te.shape)  # expect 26 or 27 columns\n",
        "\n",
        "# Drop trailing all-NaN column if present\n",
        "def drop_trailer(df):\n",
        "    if df.shape[1] >= 27 and df.iloc[:, -1].isna().all():\n",
        "        return df.iloc[:, :-1]\n",
        "    return df\n",
        "\n",
        "raw_tr = drop_trailer(raw_tr)\n",
        "raw_te = drop_trailer(raw_te)\n",
        "\n",
        "# Assign standard 26 names\n",
        "cols = [\"engine\",\"cycle\"] + [f\"op{i}\" for i in range(1,4)] + [f\"s{i}\" for i in range(1,22)]\n",
        "assert raw_tr.shape[1] == 26 and raw_te.shape[1] == 26, \"Expected 26 columns after cleanup\"\n",
        "train = raw_tr.copy(); train.columns = cols\n",
        "test  = raw_te.copy(); test.columns  = cols\n",
        "\n",
        "# Load RUL (one value per test engine)\n",
        "rul_add = pd.read_csv(RUL, delim_whitespace=True, header=None).values.flatten()\n",
        "print(\"Engines train/test:\", train.engine.nunique(), test.engine.nunique(), \"RUL entries:\", len(rul_add))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otZ3LeMDCHOE",
        "outputId": "4a657fd0-bb21-4900-ced4-f77993e08c39"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw shapes: (20631, 26) (13096, 26)\n",
            "Engines train/test: 100 100 RUL entries: 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1021833924.py:9: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  raw_tr = pd.read_csv(TRAIN, delim_whitespace=True, header=None)\n",
            "/tmp/ipython-input-1021833924.py:10: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  raw_te = pd.read_csv(TEST,  delim_whitespace=True, header=None)\n",
            "/tmp/ipython-input-1021833924.py:29: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  rul_add = pd.read_csv(RUL, delim_whitespace=True, header=None).values.flatten()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Features: all numeric except engine & cycle\n",
        "feat = [c for c in train.columns if c not in [\"engine\",\"cycle\"] and np.issubdtype(train[c].dtype, np.number)]\n",
        "assert len(feat) > 0, \"No numeric feature columns found\"\n",
        "\n",
        "# Scale with train only\n",
        "scaler = MinMaxScaler()\n",
        "train[feat] = scaler.fit_transform(train[feat])\n",
        "test[feat]  = scaler.transform(test[feat])\n",
        "\n",
        "# Train labels (piecewise cap)\n",
        "def train_labels(df, cap=125):\n",
        "    last = df.groupby(\"engine\")[\"cycle\"].max()\n",
        "    out = df.set_index(\"engine\").join(last.rename(\"last\"))\n",
        "    return (out[\"last\"] - out[\"cycle\"]).clip(upper=cap).values\n",
        "\n",
        "# Test labels using RUL file (standard FD001 protocol)\n",
        "def test_labels(df, rul_add, cap=125):\n",
        "    y = np.zeros(len(df), dtype=np.float32)\n",
        "    for i, (eng, g) in enumerate(df.groupby(\"engine\")):\n",
        "        cmax = g[\"cycle\"].max()\n",
        "        y[g.index] = np.clip((cmax - g[\"cycle\"]).values + rul_add[i], 0, cap)\n",
        "    return y\n",
        "\n",
        "y_tr_all = train_labels(train, 125)\n",
        "y_te_all = test_labels(test,  rul_add, 125)\n",
        "\n",
        "# Build 30-step windows\n",
        "def make_windows(df, y_all, window=30):\n",
        "    X, y = [], []\n",
        "    for eng, g in df.groupby(\"engine\"):\n",
        "        g = g.sort_values(\"cycle\")\n",
        "        F = g[feat].values\n",
        "        L = y_all[g.index]\n",
        "        if len(F) < window: continue\n",
        "        for i in range(len(F) - window + 1):\n",
        "            X.append(F[i:i+window]); y.append(L[i+window-1])\n",
        "    return np.array(X), np.array(y).reshape(-1,1)\n",
        "\n",
        "X_train, y_train = make_windows(train, y_tr_all, 30)\n",
        "X_test,  y_test  = make_windows(test,  y_te_all,  30)\n",
        "print(\"Shapes:\", X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FX8kKnICaQs",
        "outputId": "e3001f7d-ffcf-4cca-e09b-695412460337"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes: (17731, 30, 24) (17731, 1) (10196, 30, 24) (10196, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import numpy as np\n",
        "\n",
        "def nasa_score(y_true, y_pred):\n",
        "    e = y_pred.flatten() - y_true.flatten()\n",
        "    late  = np.where(e >= 0, np.exp(e/10.0) - 1.0, 0.0)\n",
        "    early = np.where(e <  0, np.exp(-e/13.0) - 1.0, 0.0)\n",
        "    return float(np.sum(late + early))\n",
        "\n",
        "inp = layers.Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
        "x = layers.LSTM(64)(inp)\n",
        "x = layers.Dense(32, activation=\"relu\")(x)\n",
        "out = layers.Dense(1)(x)\n",
        "model = models.Model(inp, out)\n",
        "model.compile(\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
        "model.fit(X_train, y_train, validation_split=0.2, epochs=12, batch_size=64, verbose=1)\n",
        "\n",
        "y_pred = model.predict(X_test, verbose=0)\n",
        "rmse = float(np.sqrt(np.mean((y_pred - y_test)**2)))\n",
        "print(\"RMSE:\", round(rmse,2), \" NASA:\", round(nasa_score(y_test, y_pred),2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igqko2tVDJRa",
        "outputId": "e207ae28-7905-4581-bc36-45715c34a20b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/12\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 22ms/step - loss: 6298.4155 - mae: 68.2878 - val_loss: 2066.1526 - val_mae: 41.1069\n",
            "Epoch 2/12\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - loss: 1794.7515 - mae: 37.7390 - val_loss: 1797.5743 - val_mae: 38.1815\n",
            "Epoch 3/12\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 18ms/step - loss: 1739.2649 - mae: 36.9886 - val_loss: 1782.2419 - val_mae: 38.0277\n",
            "Epoch 4/12\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - loss: 1681.7306 - mae: 36.3741 - val_loss: 1135.7322 - val_mae: 27.1445\n",
            "Epoch 5/12\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 520.1254 - mae: 18.6598 - val_loss: 283.0054 - val_mae: 12.8006\n",
            "Epoch 6/12\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - loss: 270.0552 - mae: 12.5637 - val_loss: 225.8870 - val_mae: 11.6458\n",
            "Epoch 7/12\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 24ms/step - loss: 241.9228 - mae: 11.6446 - val_loss: 206.8300 - val_mae: 11.1089\n",
            "Epoch 8/12\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 222.8866 - mae: 11.1388 - val_loss: 196.6019 - val_mae: 10.7934\n",
            "Epoch 9/12\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - loss: 214.5455 - mae: 10.8951 - val_loss: 193.0518 - val_mae: 10.7100\n",
            "Epoch 10/12\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - loss: 204.2076 - mae: 10.5909 - val_loss: 185.7999 - val_mae: 10.3396\n",
            "Epoch 11/12\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 198.5635 - mae: 10.3921 - val_loss: 182.6517 - val_mae: 10.1681\n",
            "Epoch 12/12\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 25ms/step - loss: 194.6180 - mae: 10.2472 - val_loss: 182.8179 - val_mae: 10.1775\n",
            "RMSE: 13.96  NASA: 36866.66\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers, models, optimizers, callbacks\n",
        "import numpy as np\n",
        "\n",
        "def build_gru(input_shape):\n",
        "    x_in = layers.Input(shape=input_shape)\n",
        "    x = layers.GRU(64)(x_in)\n",
        "    x = layers.Dense(32, activation=\"relu\")(x)\n",
        "    y = layers.Dense(1)(x)\n",
        "    return models.Model(x_in, y)\n",
        "\n",
        "gru = build_gru((X_train.shape[1], X_train.shape[2]))\n",
        "gru.compile(optimizers.Adam(1e-3), loss=\"mse\", metrics=[\"mae\"])\n",
        "es = callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
        "gru.fit(X_train, y_train, validation_split=0.2, epochs=15, batch_size=64, verbose=1, callbacks=[es])\n",
        "\n",
        "y_pred_gru = gru.predict(X_test, verbose=0)\n",
        "rmse_gru = float(np.sqrt(np.mean((y_pred_gru - y_test)**2)))\n",
        "score_gru = nasa_score(y_test, y_pred_gru)\n",
        "print(\"GRU RMSE:\", round(rmse_gru,2), \" NASA:\", round(score_gru,2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIx6LEghEwKu",
        "outputId": "12c2b546-854d-482e-a24a-74b59e613ac2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - loss: 5927.6460 - mae: 65.8758 - val_loss: 1846.6942 - val_mae: 38.9140\n",
            "Epoch 2/15\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 27ms/step - loss: 1729.9128 - mae: 36.9869 - val_loss: 1625.7450 - val_mae: 36.3573\n",
            "Epoch 3/15\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - loss: 1113.8534 - mae: 28.7421 - val_loss: 297.6480 - val_mae: 14.5449\n",
            "Epoch 4/15\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 27ms/step - loss: 309.5223 - mae: 13.6507 - val_loss: 246.7705 - val_mae: 12.7950\n",
            "Epoch 5/15\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - loss: 214.8545 - mae: 10.9955 - val_loss: 220.7615 - val_mae: 11.7702\n",
            "Epoch 6/15\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - loss: 195.0318 - mae: 10.3154 - val_loss: 205.3087 - val_mae: 11.1456\n",
            "Epoch 7/15\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 26ms/step - loss: 187.4049 - mae: 10.0101 - val_loss: 192.9319 - val_mae: 10.6750\n",
            "Epoch 8/15\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - loss: 183.6974 - mae: 9.8550 - val_loss: 182.0803 - val_mae: 10.2351\n",
            "Epoch 9/15\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 35ms/step - loss: 180.6187 - mae: 9.7343 - val_loss: 176.9387 - val_mae: 9.9858\n",
            "Epoch 10/15\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - loss: 177.8511 - mae: 9.6291 - val_loss: 173.9962 - val_mae: 9.8405\n",
            "Epoch 11/15\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - loss: 175.6204 - mae: 9.5473 - val_loss: 171.8856 - val_mae: 9.7288\n",
            "Epoch 12/15\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - loss: 173.7647 - mae: 9.4803 - val_loss: 170.3905 - val_mae: 9.6468\n",
            "Epoch 13/15\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 24ms/step - loss: 172.2102 - mae: 9.4230 - val_loss: 169.3263 - val_mae: 9.5874\n",
            "Epoch 14/15\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - loss: 170.7542 - mae: 9.3705 - val_loss: 168.4566 - val_mae: 9.5361\n",
            "Epoch 15/15\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - loss: 169.5439 - mae: 9.3240 - val_loss: 168.0183 - val_mae: 9.5034\n",
            "GRU RMSE: 13.48  NASA: 35966.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from tensorflow.keras import layers, models, backend as K, optimizers, callbacks\n",
        "# import numpy as np\n",
        "\n",
        "# def build_dual_sensor_time(input_shape, enc_units=128, drop=0.2):\n",
        "#     x_in = layers.Input(shape=input_shape)                           # (B, T, Fin)\n",
        "\n",
        "#     # Sensor attention over input channels\n",
        "#     # Learn per-timestep sensor scores and then average across time to get a stable per-window sensor weight\n",
        "#     s_scores = layers.TimeDistributed(layers.Dense(1, activation=\"tanh\"))(x_in)   # (B, T, 1) per feature vector? -> adjust:\n",
        "#     # Better: compute sensor scores from time-averaged inputs\n",
        "#     x_mean = layers.Lambda(lambda x: K.mean(x, axis=1))(x_in)                     # (B, Fin)\n",
        "#     a = layers.Dense(64, activation=\"tanh\")(x_mean)                                # (B, 64)\n",
        "#     alpha = layers.Dense(input_shape[-1], activation=\"softmax\", name=\"alpha\")(a)  # (B, Fin)\n",
        "#     alpha_e = layers.Lambda(lambda a: K.expand_dims(a, axis=1))(alpha)            # (B, 1, Fin)\n",
        "#     x_w = layers.Multiply()([x_in, alpha_e])                                      # (B, T, Fin) sensor-weighted inputs\n",
        "\n",
        "#     # LSTM encoder on sensor-weighted inputs\n",
        "#     H = layers.LSTM(enc_units, return_sequences=True)(x_w)                         # (B, T, enc)\n",
        "#     H = layers.Dropout(drop)(H)\n",
        "\n",
        "#     # Temporal attention over T\n",
        "#     e_t = layers.Dense(1, activation=\"tanh\")(H)                                    # (B, T, 1)\n",
        "#     e_t = layers.Flatten()(e_t)                                                    # (B, T)\n",
        "#     beta = layers.Activation(\"softmax\", name=\"beta\")(e_t)                          # (B, T)\n",
        "#     beta_e = layers.Lambda(lambda b: K.expand_dims(b, -1))(beta)                   # (B, T, 1)\n",
        "#     H_t = layers.Multiply()([H, beta_e])                                           # (B, T, enc)\n",
        "\n",
        "#     context = layers.Lambda(lambda x: K.sum(x, axis=1))(H_t)                       # (B, enc)\n",
        "#     x = layers.Dense(64, activation=\"relu\")(context)\n",
        "#     y = layers.Dense(1, name=\"rul\")(x)\n",
        "#     return models.Model(x_in, [y, alpha, beta])\n",
        "\n",
        "# dual_st = build_dual_sensor_time((X_train.shape[1], X_train.shape[2]), enc_units=128, drop=0.2)\n",
        "# dual_st.compile(optimizers.Adam(1e-3),\n",
        "#                 loss={\"rul\":\"mse\",\"alpha\":lambda yt,yp: 0.0*yp, \"beta\":lambda yt,yp: 0.0*yp},\n",
        "#                 loss_weights={\"rul\":1.0,\"alpha\":0.0,\"beta\":0.0},\n",
        "#                 metrics={\"rul\":[\"mae\"]})\n",
        "# es = callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
        "# hist = dual_st.fit(X_train, {\"rul\": y_train, \"alpha\": np.zeros((X_train.shape[0], X_train.shape[2])),\n",
        "#                              \"beta\": np.zeros((X_train.shape[0], X_train.shape[1]))},\n",
        "#                    validation_split=0.2, epochs=15, batch_size=64, verbose=1, callbacks=[es])\n",
        "\n",
        "# y_pred_dual, alpha_dual, beta_dual = dual_st.predict(X_test, verbose=0)\n",
        "# rmse_dual = float(np.sqrt(np.mean((y_pred_dual - y_test)**2)))\n",
        "# score_dual = nasa_score(y_test, y_pred_dual)\n",
        "# print(\"Dual(sensor→time) RMSE:\", round(rmse_dual,2), \" NASA:\", round(score_dual,2))\n"
      ],
      "metadata": {
        "id": "938QOGaxFR8a"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers, models, backend as K, optimizers, callbacks\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "def build_dual_sensor_time_residual(input_shape, enc_units=128, eps=0.3, drop=0.2):\n",
        "    Fin = input_shape[-1]\n",
        "    x_in = layers.Input(shape=input_shape)                           # (B, T, Fin)\n",
        "\n",
        "    # Sensor attention over inputs (per-window)\n",
        "    x_mean = layers.Lambda(lambda x: K.mean(x, axis=1))(x_in)        # (B, Fin)\n",
        "    a = layers.Dense(64, activation=\"tanh\")(x_mean)                   # (B, 64)\n",
        "    alpha = layers.Dense(Fin, activation=\"softmax\", name=\"alpha\")(a)  # (B, Fin)\n",
        "    alpha_e = layers.Lambda(lambda a: K.expand_dims(a, axis=1))(alpha)# (B, 1, Fin)\n",
        "\n",
        "    # Residual gate: x_w = x_in * (1 + eps*(alpha - 1/Fin))\n",
        "    one_over = 1.0/Fin\n",
        "    def residual_gate(inputs):\n",
        "        x, aE = inputs\n",
        "        return x * (1.0 + eps*(aE - one_over))\n",
        "    x_w = layers.Lambda(residual_gate)([x_in, alpha_e])               # (B, T, Fin)\n",
        "\n",
        "    # Encoder\n",
        "    H = layers.LSTM(enc_units, return_sequences=True)(x_w)            # (B, T, enc)\n",
        "    H = layers.Dropout(drop)(H)\n",
        "\n",
        "    # Temporal attention\n",
        "    e_t = layers.Dense(1, activation=\"tanh\")(H)                       # (B, T, 1)\n",
        "    e_t = layers.Flatten()(e_t)                                       # (B, T)\n",
        "    beta = layers.Activation(\"softmax\", name=\"beta\")(e_t)             # (B, T)\n",
        "    beta_e = layers.Lambda(lambda b: K.expand_dims(b, -1))(beta)      # (B, T, 1)\n",
        "    H_t = layers.Multiply()([H, beta_e])                              # (B, T, enc)\n",
        "\n",
        "    context = layers.Lambda(lambda x: K.sum(x, axis=1))(H_t)          # (B, enc)\n",
        "    x = layers.Dense(64, activation=\"relu\")(context)\n",
        "    y = layers.Dense(1, name=\"rul\")(x)\n",
        "    return models.Model(x_in, [y, alpha, beta])\n",
        "\n",
        "dual_res = build_dual_sensor_time_residual((X_train.shape[1], X_train.shape[2]), enc_units=192, eps=0.3, drop=0.25)\n",
        "dual_res.compile(optimizers.Adam(8e-4),\n",
        "                 loss={\"rul\":\"mse\",\"alpha\":lambda yt,yp: 0.0*yp,\"beta\":lambda yt,yp: 0.0*yp},\n",
        "                 loss_weights={\"rul\":1.0,\"alpha\":0.0,\"beta\":0.0},\n",
        "                 metrics={\"rul\":[\"mae\"]})\n",
        "es = callbacks.EarlyStopping(monitor=\"val_loss\", patience=6, restore_best_weights=True)\n",
        "dual_res.fit(X_train, {\"rul\": y_train, \"alpha\": np.zeros((X_train.shape[0], X_train.shape[2])),\n",
        "                       \"beta\": np.zeros((X_train.shape[0], X_train.shape[1]))},\n",
        "             validation_split=0.2, epochs=25, batch_size=64, verbose=1, callbacks=[es])\n",
        "\n",
        "y_pred_res, alpha_res, beta_res = dual_res.predict(X_test, verbose=0)\n",
        "rmse_res = float(np.sqrt(np.mean((y_pred_res - y_test)**2)))\n",
        "score_res = nasa_score(y_test, y_pred_res)\n",
        "print(\"Dual residual RMSE:\", round(rmse_res,2), \" NASA:\", round(score_res,2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pebA40cXKj5X",
        "outputId": "26396574-faaf-46f0-8b8a-7c418a3d5c3e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 96ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 4129.6167 - rul_loss: 4129.6040 - rul_mae: 53.7144 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 1710.0992 - val_rul_loss: 1733.4183 - val_rul_mae: 37.3789\n",
            "Epoch 2/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 94ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 1355.0166 - rul_loss: 1355.0167 - rul_mae: 32.1811 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 697.3523 - val_rul_loss: 691.7470 - val_rul_mae: 21.5149\n",
            "Epoch 3/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 93ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 640.5833 - rul_loss: 640.5856 - rul_mae: 20.3013 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 538.3172 - val_rul_loss: 534.1983 - val_rul_mae: 19.2770\n",
            "Epoch 4/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 96ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 510.3156 - rul_loss: 510.3164 - rul_mae: 17.6975 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 455.6377 - val_rul_loss: 451.0901 - val_rul_mae: 17.9522\n",
            "Epoch 5/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 91ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 397.6957 - rul_loss: 397.6991 - rul_mae: 15.5163 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 345.4723 - val_rul_loss: 342.3109 - val_rul_mae: 14.9405\n",
            "Epoch 6/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 101ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 340.7787 - rul_loss: 340.7800 - rul_mae: 14.1792 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 315.4923 - val_rul_loss: 312.3235 - val_rul_mae: 14.0153\n",
            "Epoch 7/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 94ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 304.3397 - rul_loss: 304.3399 - rul_mae: 13.1812 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 282.0651 - val_rul_loss: 279.2973 - val_rul_mae: 13.4702\n",
            "Epoch 8/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 92ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 278.6124 - rul_loss: 278.6123 - rul_mae: 12.5118 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 243.3942 - val_rul_loss: 240.9583 - val_rul_mae: 11.8658\n",
            "Epoch 9/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 101ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 262.8255 - rul_loss: 262.8254 - rul_mae: 12.0814 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 238.0057 - val_rul_loss: 235.6033 - val_rul_mae: 11.5155\n",
            "Epoch 10/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 91ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 242.5919 - rul_loss: 242.5914 - rul_mae: 11.5871 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 238.9310 - val_rul_loss: 236.5331 - val_rul_mae: 11.5703\n",
            "Epoch 11/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 96ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 242.3059 - rul_loss: 242.3047 - rul_mae: 11.5916 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 242.8590 - val_rul_loss: 240.5135 - val_rul_mae: 11.5730\n",
            "Epoch 12/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 96ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 230.2571 - rul_loss: 230.2559 - rul_mae: 11.2656 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 238.0310 - val_rul_loss: 235.7309 - val_rul_mae: 11.5613\n",
            "Epoch 13/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 91ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 229.3993 - rul_loss: 229.3982 - rul_mae: 11.2621 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 222.5665 - val_rul_loss: 220.3773 - val_rul_mae: 11.3328\n",
            "Epoch 14/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 96ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 215.2256 - rul_loss: 215.2247 - rul_mae: 10.8616 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 225.0998 - val_rul_loss: 222.8505 - val_rul_mae: 11.0372\n",
            "Epoch 15/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 90ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 212.8228 - rul_loss: 212.8220 - rul_mae: 10.7973 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 230.4028 - val_rul_loss: 228.1275 - val_rul_mae: 11.0655\n",
            "Epoch 16/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 101ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 206.5645 - rul_loss: 206.5636 - rul_mae: 10.6161 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 226.8478 - val_rul_loss: 224.6864 - val_rul_mae: 11.1552\n",
            "Epoch 17/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 90ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 204.1098 - rul_loss: 204.1090 - rul_mae: 10.5354 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 222.4411 - val_rul_loss: 220.2947 - val_rul_mae: 11.0993\n",
            "Epoch 18/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 96ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 204.6889 - rul_loss: 204.6883 - rul_mae: 10.5832 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 207.3357 - val_rul_loss: 205.3334 - val_rul_mae: 11.0552\n",
            "Epoch 19/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 92ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 202.0146 - rul_loss: 202.0140 - rul_mae: 10.5233 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 205.2625 - val_rul_loss: 203.1885 - val_rul_mae: 11.0178\n",
            "Epoch 20/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 100ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 198.9109 - rul_loss: 198.9106 - rul_mae: 10.4103 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 204.8887 - val_rul_loss: 202.8439 - val_rul_mae: 10.8213\n",
            "Epoch 21/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 96ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 209.0776 - rul_loss: 209.0772 - rul_mae: 10.6488 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 203.6392 - val_rul_loss: 201.6005 - val_rul_mae: 10.6759\n",
            "Epoch 22/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 100ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 199.5003 - rul_loss: 199.5000 - rul_mae: 10.3596 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 197.7098 - val_rul_loss: 195.7111 - val_rul_mae: 10.7165\n",
            "Epoch 23/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 102ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 198.3069 - rul_loss: 198.3069 - rul_mae: 10.3785 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 206.4929 - val_rul_loss: 204.4097 - val_rul_mae: 11.2845\n",
            "Epoch 24/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 93ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 193.9434 - rul_loss: 193.9431 - rul_mae: 10.2155 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 200.1548 - val_rul_loss: 198.1385 - val_rul_mae: 10.9769\n",
            "Epoch 25/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 94ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 191.4131 - rul_loss: 191.4129 - rul_mae: 10.1321 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 196.3395 - val_rul_loss: 194.3733 - val_rul_mae: 10.7901\n",
            "Dual residual RMSE: 14.13  NASA: 37080.18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd, matplotlib.pyplot as plt, numpy as np\n",
        "\n",
        "# Assuming y_pred_baseline, y_pred_gru, y_pred_res exist (else compute)\n",
        "# Save metrics\n",
        "df = pd.DataFrame([\n",
        "  {\"model\":\"LSTM\",\"rmse\":13.96,\"nasa\":36866.66},\n",
        "  {\"model\":\"GRU\",\"rmse\":13.48,\"nasa\":35966.99},\n",
        "  {\"model\":\"DualResidual\",\"rmse\":14.13,\"nasa\":37080.18},\n",
        "])\n",
        "df.to_csv(\"fd001_metrics.csv\", index=False)\n",
        "\n",
        "# Quick plot (dual vs truth)\n",
        "# y_pred_res computed earlier as y_pred_res\n",
        "plt.figure(figsize=(6,3))\n",
        "plt.plot(y_test[:200], label=\"true\")\n",
        "plt.plot(y_pred_res[:200], label=\"dual\")\n",
        "plt.legend(); plt.tight_layout(); plt.savefig(\"fd001_pred_dual.png\"); plt.close()\n",
        "print(\"Saved fd001_metrics.csv and fd001_pred_dual.png\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6CwVCTPZo1A",
        "outputId": "8168cf83-8270-4df4-8b08-67a7b03b1aec"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved fd001_metrics.csv and fd001_pred_dual.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dual_res.compile(optimizers.Adam(6e-4),\n",
        "                 loss={\"rul\":\"mse\",\"alpha\":lambda yt,yp:0.0*yp,\"beta\":lambda yt,yp:0.0*yp},\n",
        "                 loss_weights={\"rul\":1.0,\"alpha\":0.0,\"beta\":0.0},\n",
        "                 metrics={\"rul\":[\"mae\"]})\n",
        "es = callbacks.EarlyStopping(monitor=\"val_loss\", patience=8, restore_best_weights=True)\n"
      ],
      "metadata": {
        "id": "td-04TmvZzhg"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "# Build simple prior p over input sensors (Fin) – adjust list per your names if needed\n",
        "Fin = X_train.shape[2]\n",
        "p = np.ones(Fin, dtype=np.float32)\n",
        "# Example heuristic upweights: s2,s3,s4,s7,s8,s9,s11,s12,s13,s14\n",
        "sensor_names = [c for c in train.columns if c not in [\"engine\",\"cycle\"]]\n",
        "for i, c in enumerate(sensor_names):\n",
        "    if c.startswith(\"s\") and int(c[1:]) in [2,3,4,7,8,9,11,12,13,14]:\n",
        "        p[i] = 1.5\n",
        "p = p / p.sum()\n",
        "lam = 0.01\n",
        "\n",
        "def alpha_prior_loss(y_true, alpha):\n",
        "    p_batch = tf.constant(p)[None, :]\n",
        "    return lam * tf.reduce_mean(tf.square(alpha - p_batch))\n",
        "\n",
        "dual_res.compile(optimizers.Adam(6e-4),\n",
        "                 loss={\"rul\":\"mse\",\"alpha\":alpha_prior_loss,\"beta\":lambda yt,yp: 0.0*yp},\n",
        "                 loss_weights={\"rul\":1.0,\"alpha\":1.0,\"beta\":0.0},\n",
        "                 metrics={\"rul\":[\"mae\"]})\n",
        "dual_res.fit(X_train, {\"rul\": y_train, \"alpha\": np.zeros((X_train.shape[0], Fin)),\n",
        "                       \"beta\": np.zeros((X_train.shape[0], X_train.shape[1]))},\n",
        "             validation_split=0.2, epochs=20, batch_size=64, verbose=1, callbacks=[es])\n",
        "\n",
        "y_pred_res2, alpha_res2, beta_res2 = dual_res.predict(X_test, verbose=0)\n",
        "rmse_res2 = float(np.sqrt(np.mean((y_pred_res2 - y_test)**2)))\n",
        "score_res2 = nasa_score(y_test, y_pred_res2)\n",
        "print(\"DualResidual+Prior RMSE:\", round(rmse_res2,2), \" NASA:\", round(score_res2,2))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2to8Z0TwZ2DV",
        "outputId": "2a1cee73-249b-492e-c655-cec6c319cb68"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 99ms/step - alpha_loss: 1.6060e-04 - beta_loss: 0.0000e+00 - loss: 202.5321 - rul_loss: 202.5311 - rul_mae: 10.4632 - val_alpha_loss: 1.6110e-04 - val_beta_loss: 0.0000e+00 - val_loss: 201.9028 - val_rul_loss: 199.8690 - val_rul_mae: 10.9118\n",
            "Epoch 2/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 92ms/step - alpha_loss: 1.5960e-04 - beta_loss: 0.0000e+00 - loss: 174.5329 - rul_loss: 174.5321 - rul_mae: 9.6764 - val_alpha_loss: 1.5992e-04 - val_beta_loss: 0.0000e+00 - val_loss: 208.4905 - val_rul_loss: 206.3776 - val_rul_mae: 11.1953\n",
            "Epoch 3/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 98ms/step - alpha_loss: 1.5758e-04 - beta_loss: 0.0000e+00 - loss: 173.2160 - rul_loss: 173.2155 - rul_mae: 9.6533 - val_alpha_loss: 1.5872e-04 - val_beta_loss: 0.0000e+00 - val_loss: 202.1676 - val_rul_loss: 200.1275 - val_rul_mae: 10.9274\n",
            "Epoch 4/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 98ms/step - alpha_loss: 1.5564e-04 - beta_loss: 0.0000e+00 - loss: 173.9058 - rul_loss: 173.9052 - rul_mae: 9.6455 - val_alpha_loss: 1.5571e-04 - val_beta_loss: 0.0000e+00 - val_loss: 193.3902 - val_rul_loss: 191.4443 - val_rul_mae: 10.5283\n",
            "Epoch 5/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 92ms/step - alpha_loss: 1.5326e-04 - beta_loss: 0.0000e+00 - loss: 170.9576 - rul_loss: 170.9568 - rul_mae: 9.5663 - val_alpha_loss: 1.5582e-04 - val_beta_loss: 0.0000e+00 - val_loss: 203.7147 - val_rul_loss: 201.6649 - val_rul_mae: 10.8357\n",
            "Epoch 6/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 98ms/step - alpha_loss: 1.5160e-04 - beta_loss: 0.0000e+00 - loss: 167.4804 - rul_loss: 167.4795 - rul_mae: 9.4492 - val_alpha_loss: 1.5110e-04 - val_beta_loss: 0.0000e+00 - val_loss: 194.9006 - val_rul_loss: 192.9430 - val_rul_mae: 10.4115\n",
            "Epoch 7/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 98ms/step - alpha_loss: 1.4848e-04 - beta_loss: 0.0000e+00 - loss: 164.6020 - rul_loss: 164.6011 - rul_mae: 9.3360 - val_alpha_loss: 1.4927e-04 - val_beta_loss: 0.0000e+00 - val_loss: 188.2271 - val_rul_loss: 186.3250 - val_rul_mae: 10.1130\n",
            "Epoch 8/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 92ms/step - alpha_loss: 1.4766e-04 - beta_loss: 0.0000e+00 - loss: 162.1833 - rul_loss: 162.1825 - rul_mae: 9.2544 - val_alpha_loss: 1.4919e-04 - val_beta_loss: 0.0000e+00 - val_loss: 194.2700 - val_rul_loss: 192.3302 - val_rul_mae: 10.5284\n",
            "Epoch 9/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 104ms/step - alpha_loss: 1.4661e-04 - beta_loss: 0.0000e+00 - loss: 163.6167 - rul_loss: 163.6161 - rul_mae: 9.3100 - val_alpha_loss: 1.4774e-04 - val_beta_loss: 0.0000e+00 - val_loss: 194.2344 - val_rul_loss: 192.2871 - val_rul_mae: 10.4341\n",
            "Epoch 10/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 97ms/step - alpha_loss: 1.4411e-04 - beta_loss: 0.0000e+00 - loss: 161.7989 - rul_loss: 161.7979 - rul_mae: 9.2730 - val_alpha_loss: 1.4237e-04 - val_beta_loss: 0.0000e+00 - val_loss: 186.0712 - val_rul_loss: 184.2060 - val_rul_mae: 9.8296\n",
            "Epoch 11/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 103ms/step - alpha_loss: 1.4010e-04 - beta_loss: 0.0000e+00 - loss: 160.2500 - rul_loss: 160.2492 - rul_mae: 9.1709 - val_alpha_loss: 1.4138e-04 - val_beta_loss: 0.0000e+00 - val_loss: 197.8239 - val_rul_loss: 195.8848 - val_rul_mae: 10.6117\n",
            "Epoch 12/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 98ms/step - alpha_loss: 1.3827e-04 - beta_loss: 0.0000e+00 - loss: 156.0195 - rul_loss: 156.0186 - rul_mae: 9.0655 - val_alpha_loss: 1.3817e-04 - val_beta_loss: 0.0000e+00 - val_loss: 190.2243 - val_rul_loss: 188.3086 - val_rul_mae: 10.1561\n",
            "Epoch 13/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 93ms/step - alpha_loss: 1.3553e-04 - beta_loss: 0.0000e+00 - loss: 155.2068 - rul_loss: 155.2058 - rul_mae: 9.0389 - val_alpha_loss: 1.3553e-04 - val_beta_loss: 0.0000e+00 - val_loss: 190.2681 - val_rul_loss: 188.3438 - val_rul_mae: 10.2126\n",
            "Epoch 14/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 99ms/step - alpha_loss: 1.3342e-04 - beta_loss: 0.0000e+00 - loss: 151.6474 - rul_loss: 151.6465 - rul_mae: 8.9483 - val_alpha_loss: 1.3382e-04 - val_beta_loss: 0.0000e+00 - val_loss: 186.5909 - val_rul_loss: 184.7036 - val_rul_mae: 9.9534\n",
            "Epoch 15/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 97ms/step - alpha_loss: 1.3205e-04 - beta_loss: 0.0000e+00 - loss: 152.0191 - rul_loss: 152.0181 - rul_mae: 8.9426 - val_alpha_loss: 1.3201e-04 - val_beta_loss: 0.0000e+00 - val_loss: 184.5139 - val_rul_loss: 182.6375 - val_rul_mae: 9.9716\n",
            "Epoch 16/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 98ms/step - alpha_loss: 1.3035e-04 - beta_loss: 0.0000e+00 - loss: 151.5771 - rul_loss: 151.5761 - rul_mae: 8.9024 - val_alpha_loss: 1.3113e-04 - val_beta_loss: 0.0000e+00 - val_loss: 185.6193 - val_rul_loss: 183.7349 - val_rul_mae: 9.8064\n",
            "Epoch 17/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 93ms/step - alpha_loss: 1.2957e-04 - beta_loss: 0.0000e+00 - loss: 149.0604 - rul_loss: 149.0594 - rul_mae: 8.8307 - val_alpha_loss: 1.3184e-04 - val_beta_loss: 0.0000e+00 - val_loss: 190.8484 - val_rul_loss: 188.9166 - val_rul_mae: 9.8938\n",
            "Epoch 18/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 98ms/step - alpha_loss: 1.3083e-04 - beta_loss: 0.0000e+00 - loss: 146.2454 - rul_loss: 146.2444 - rul_mae: 8.7490 - val_alpha_loss: 1.3273e-04 - val_beta_loss: 0.0000e+00 - val_loss: 191.6145 - val_rul_loss: 189.6661 - val_rul_mae: 10.1574\n",
            "Epoch 19/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 99ms/step - alpha_loss: 1.3101e-04 - beta_loss: 0.0000e+00 - loss: 146.3985 - rul_loss: 146.3974 - rul_mae: 8.7620 - val_alpha_loss: 1.3271e-04 - val_beta_loss: 0.0000e+00 - val_loss: 191.1640 - val_rul_loss: 189.2699 - val_rul_mae: 9.9974\n",
            "Epoch 20/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 100ms/step - alpha_loss: 1.3207e-04 - beta_loss: 0.0000e+00 - loss: 146.6254 - rul_loss: 146.6243 - rul_mae: 8.7482 - val_alpha_loss: 1.3491e-04 - val_beta_loss: 0.0000e+00 - val_loss: 189.4480 - val_rul_loss: 187.5703 - val_rul_mae: 9.8142\n",
            "DualResidual+Prior RMSE: 14.16  NASA: 48700.27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Residual dual attention model: sensor -> time\n",
        "from tensorflow.keras import layers, models, backend as K, optimizers, callbacks\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "def build_dual_sensor_time_residual(input_shape, enc_units=192, eps=0.25, drop=0.25):\n",
        "    Fin = input_shape[-1]\n",
        "    x_in = layers.Input(shape=input_shape)                                  # (B, T, Fin)\n",
        "\n",
        "    # Sensor attention over input channels (per-window)\n",
        "    x_mean = layers.Lambda(lambda x: K.mean(x, axis=1))(x_in)               # (B, Fin)\n",
        "    a = layers.Dense(64, activation=\"tanh\")(x_mean)                          # (B, 64)\n",
        "    alpha = layers.Dense(Fin, activation=\"softmax\", name=\"alpha\")(a)         # (B, Fin)\n",
        "    alpha_e = layers.Lambda(lambda a: K.expand_dims(a, axis=1))(alpha)       # (B, 1, Fin)\n",
        "\n",
        "    # Residual sensor gate: x_w = x_in * (1 + eps*(alpha - 1/Fin))\n",
        "    one_over = 1.0 / Fin\n",
        "    def residual_gate(inputs):\n",
        "        x, aE = inputs\n",
        "        return x * (1.0 + eps * (aE - one_over))\n",
        "    x_w = layers.Lambda(residual_gate)([x_in, alpha_e])                      # (B, T, Fin)\n",
        "\n",
        "    # Encoder\n",
        "    H = layers.LSTM(enc_units, return_sequences=True)(x_w)                   # (B, T, enc)\n",
        "    H = layers.Dropout(drop)(H)\n",
        "\n",
        "    # Temporal attention over T\n",
        "    e_t = layers.Dense(1, activation=\"tanh\")(H)                              # (B, T, 1)\n",
        "    e_t = layers.Flatten()(e_t)                                              # (B, T)\n",
        "    beta = layers.Activation(\"softmax\", name=\"beta\")(e_t)                    # (B, T)\n",
        "    beta_e = layers.Lambda(lambda b: K.expand_dims(b, -1))(beta)             # (B, T, 1)\n",
        "    H_t = layers.Multiply()([H, beta_e])                                     # (B, T, enc)\n",
        "\n",
        "    context = layers.Lambda(lambda x: K.sum(x, axis=1))(H_t)                 # (B, enc)\n",
        "    x = layers.Dense(64, activation=\"relu\")(context)\n",
        "    y = layers.Dense(1, name=\"rul\")(x)\n",
        "    return models.Model(inputs=x_in, outputs=[y, alpha, beta])\n",
        "\n",
        "# Build and compile with a slightly smaller LR and longer patience\n",
        "input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "dual_res = build_dual_sensor_time_residual(input_shape, enc_units=192, eps=0.25, drop=0.25)\n",
        "\n",
        "dual_res.compile(\n",
        "    optimizers.Adam(6e-4),\n",
        "    loss={\"rul\":\"mse\", \"alpha\": lambda yt,yp: 0.0*yp, \"beta\": lambda yt,yp: 0.0*yp},\n",
        "    loss_weights={\"rul\":1.0, \"alpha\":0.0, \"beta\":0.0},\n",
        "    metrics={\"rul\": [\"mae\"]}\n",
        ")\n",
        "\n",
        "es = callbacks.EarlyStopping(monitor=\"val_loss\", patience=8, restore_best_weights=True)\n",
        "\n",
        "hist = dual_res.fit(\n",
        "    X_train,\n",
        "    {\"rul\": y_train, \"alpha\": np.zeros((X_train.shape[0], X_train.shape[2])), \"beta\": np.zeros((X_train.shape[0], X_train.shape[1]))},\n",
        "    validation_split=0.2, epochs=25, batch_size=64, verbose=1, callbacks=[es]\n",
        ")\n",
        "\n",
        "y_pred_res, alpha_res, beta_res = dual_res.predict(X_test, verbose=0)\n",
        "rmse_res = float(np.sqrt(np.mean((y_pred_res - y_test)**2)))\n",
        "print(\"DualResidual RMSE:\", round(rmse_res,2), \" NASA:\", round(nasa_score(y_test, y_pred_res),2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3M9cg3plqEn",
        "outputId": "29b32b25-c5d4-4c09-d619-2c928000b57d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 97ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 4385.8799 - rul_loss: 4385.8647 - rul_mae: 55.5342 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 1725.8138 - val_rul_loss: 1747.9623 - val_rul_mae: 37.5963\n",
            "Epoch 2/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 92ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 1314.0535 - rul_loss: 1314.0496 - rul_mae: 31.5670 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 817.4664 - val_rul_loss: 809.0831 - val_rul_mae: 23.5988\n",
            "Epoch 3/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 101ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 510.3823 - rul_loss: 510.3828 - rul_mae: 17.9470 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 453.4492 - val_rul_loss: 448.8765 - val_rul_mae: 17.1387\n",
            "Epoch 4/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 96ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 394.3531 - rul_loss: 394.3550 - rul_mae: 15.4450 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 327.8288 - val_rul_loss: 324.5153 - val_rul_mae: 14.4657\n",
            "Epoch 5/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 91ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 333.0591 - rul_loss: 333.0594 - rul_mae: 14.1612 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 296.9982 - val_rul_loss: 294.0312 - val_rul_mae: 13.4046\n",
            "Epoch 6/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 96ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 301.7325 - rul_loss: 301.7327 - rul_mae: 13.3487 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 272.0742 - val_rul_loss: 269.4392 - val_rul_mae: 12.2182\n",
            "Epoch 7/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 92ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 271.8943 - rul_loss: 271.8933 - rul_mae: 12.5434 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 259.8677 - val_rul_loss: 257.2603 - val_rul_mae: 11.8290\n",
            "Epoch 8/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 96ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 270.9208 - rul_loss: 270.9194 - rul_mae: 12.4565 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 250.8961 - val_rul_loss: 248.3839 - val_rul_mae: 12.3784\n",
            "Epoch 9/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 90ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 254.1681 - rul_loss: 254.1666 - rul_mae: 11.9754 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 253.6214 - val_rul_loss: 251.1196 - val_rul_mae: 11.7993\n",
            "Epoch 10/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 96ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 240.2507 - rul_loss: 240.2496 - rul_mae: 11.6036 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 231.5994 - val_rul_loss: 229.2636 - val_rul_mae: 11.5044\n",
            "Epoch 11/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 96ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 228.5539 - rul_loss: 228.5527 - rul_mae: 11.2250 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 225.3539 - val_rul_loss: 223.1109 - val_rul_mae: 11.1493\n",
            "Epoch 12/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 96ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 226.0196 - rul_loss: 226.0184 - rul_mae: 11.1551 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 217.8252 - val_rul_loss: 215.7105 - val_rul_mae: 11.1038\n",
            "Epoch 13/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 96ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 220.6476 - rul_loss: 220.6467 - rul_mae: 11.0353 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 226.7946 - val_rul_loss: 224.6374 - val_rul_mae: 11.4049\n",
            "Epoch 14/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 95ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 216.2497 - rul_loss: 216.2483 - rul_mae: 10.8977 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 216.5233 - val_rul_loss: 214.4596 - val_rul_mae: 10.8420\n",
            "Epoch 15/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 96ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 211.8467 - rul_loss: 211.8454 - rul_mae: 10.7396 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 211.3072 - val_rul_loss: 209.2024 - val_rul_mae: 10.6309\n",
            "Epoch 16/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 93ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 211.0741 - rul_loss: 211.0730 - rul_mae: 10.6831 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 204.8538 - val_rul_loss: 202.8296 - val_rul_mae: 10.8326\n",
            "Epoch 17/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 94ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 211.7115 - rul_loss: 211.7104 - rul_mae: 10.6722 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 216.6238 - val_rul_loss: 214.5098 - val_rul_mae: 11.0453\n",
            "Epoch 18/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 96ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 202.7151 - rul_loss: 202.7150 - rul_mae: 10.4753 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 205.7041 - val_rul_loss: 203.6880 - val_rul_mae: 10.9781\n",
            "Epoch 19/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 90ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 201.5755 - rul_loss: 201.5745 - rul_mae: 10.3938 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 202.1335 - val_rul_loss: 200.1355 - val_rul_mae: 10.5708\n",
            "Epoch 20/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 96ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 198.3459 - rul_loss: 198.3453 - rul_mae: 10.3308 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 202.8862 - val_rul_loss: 200.8558 - val_rul_mae: 10.9353\n",
            "Epoch 21/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 90ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 199.4316 - rul_loss: 199.4310 - rul_mae: 10.3576 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 207.7126 - val_rul_loss: 205.6667 - val_rul_mae: 11.1062\n",
            "Epoch 22/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 96ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 190.2753 - rul_loss: 190.2747 - rul_mae: 10.1085 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 199.4636 - val_rul_loss: 197.5162 - val_rul_mae: 10.9373\n",
            "Epoch 23/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 96ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 191.8382 - rul_loss: 191.8375 - rul_mae: 10.1133 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 197.3702 - val_rul_loss: 195.3828 - val_rul_mae: 10.7126\n",
            "Epoch 24/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 96ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 187.2016 - rul_loss: 187.2008 - rul_mae: 9.9876 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 198.1836 - val_rul_loss: 196.2330 - val_rul_mae: 10.7346\n",
            "Epoch 25/25\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 90ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 182.6987 - rul_loss: 182.6981 - rul_mae: 9.8604 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 202.3263 - val_rul_loss: 200.3120 - val_rul_mae: 10.9461\n",
            "DualResidual RMSE: 14.0  NASA: 35900.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple sensor prior p over Fin channels (slightly favor core temp/pressure-like sensors)\n",
        "Fin = X_train.shape[2]\n",
        "sensor_names = [c for c in train.columns if c not in [\"engine\",\"cycle\"]]\n",
        "p = np.ones(Fin, dtype=np.float32)\n",
        "for i, c in enumerate(sensor_names):\n",
        "    if c.startswith(\"s\") and int(c[1:]) in [2,3,4,7,8,9,11,12,13,14]:\n",
        "        p[i] = 1.5\n",
        "p = p / p.sum()\n",
        "\n",
        "lam = 0.0025  # start tiny\n",
        "\n",
        "def alpha_prior_loss(y_true, alpha):\n",
        "    p_batch = tf.constant(p)[None, :]\n",
        "    return lam * tf.reduce_mean(tf.square(alpha - p_batch))\n",
        "\n",
        "dual_res.compile(\n",
        "    optimizers.Adam(6e-4),\n",
        "    loss={\"rul\":\"mse\", \"alpha\": alpha_prior_loss, \"beta\": lambda yt,yp: 0.0*yp},\n",
        "    loss_weights={\"rul\":1.0, \"alpha\":1.0, \"beta\":0.0},\n",
        "    metrics={\"rul\": [\"mae\"]}\n",
        ")\n",
        "\n",
        "es = callbacks.EarlyStopping(monitor=\"val_loss\", patience=8, restore_best_weights=True)\n",
        "\n",
        "hist = dual_res.fit(\n",
        "    X_train,\n",
        "    {\"rul\": y_train, \"alpha\": np.zeros((X_train.shape[0], Fin)), \"beta\": np.zeros((X_train.shape[0], X_train.shape[1]))},\n",
        "    validation_split=0.2, epochs=20, batch_size=64, verbose=1, callbacks=[es]\n",
        ")\n",
        "\n",
        "y_pred_pr, alpha_pr, beta_pr = dual_res.predict(X_test, verbose=0)\n",
        "rmse_pr = float(np.sqrt(np.mean((y_pred_pr - y_test)**2)))\n",
        "nasa_pr = nasa_score(y_test, y_pred_pr)\n",
        "print(\"DualResidual+Prior RMSE:\", round(rmse_pr,2), \" NASA:\", round(nasa_pr,2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXX65IMmoyUm",
        "outputId": "41566949-0837-4a62-dd3d-fd0a92813179"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 100ms/step - alpha_loss: 2.4839e-05 - beta_loss: 0.0000e+00 - loss: 202.8336 - rul_loss: 202.8331 - rul_mae: 10.4638 - val_alpha_loss: 2.7033e-05 - val_beta_loss: 0.0000e+00 - val_loss: 201.6534 - val_rul_loss: 199.7125 - val_rul_mae: 10.8914\n",
            "Epoch 2/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 95ms/step - alpha_loss: 2.5464e-05 - beta_loss: 0.0000e+00 - loss: 189.7561 - rul_loss: 189.7551 - rul_mae: 10.1312 - val_alpha_loss: 2.8248e-05 - val_beta_loss: 0.0000e+00 - val_loss: 199.6233 - val_rul_loss: 197.7823 - val_rul_mae: 10.2992\n",
            "Epoch 3/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 96ms/step - alpha_loss: 2.5351e-05 - beta_loss: 0.0000e+00 - loss: 185.7125 - rul_loss: 185.7117 - rul_mae: 9.9121 - val_alpha_loss: 2.5944e-05 - val_beta_loss: 0.0000e+00 - val_loss: 192.4710 - val_rul_loss: 190.5989 - val_rul_mae: 10.3787\n",
            "Epoch 4/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 97ms/step - alpha_loss: 2.4984e-05 - beta_loss: 0.0000e+00 - loss: 182.3325 - rul_loss: 182.3317 - rul_mae: 9.8413 - val_alpha_loss: 2.7752e-05 - val_beta_loss: 0.0000e+00 - val_loss: 193.5867 - val_rul_loss: 191.6432 - val_rul_mae: 10.4557\n",
            "Epoch 5/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 109ms/step - alpha_loss: 2.6637e-05 - beta_loss: 0.0000e+00 - loss: 181.2477 - rul_loss: 181.2470 - rul_mae: 9.7833 - val_alpha_loss: 3.0344e-05 - val_beta_loss: 0.0000e+00 - val_loss: 197.2011 - val_rul_loss: 195.3149 - val_rul_mae: 10.8489\n",
            "Epoch 6/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 97ms/step - alpha_loss: 2.7377e-05 - beta_loss: 0.0000e+00 - loss: 176.5455 - rul_loss: 176.5450 - rul_mae: 9.6445 - val_alpha_loss: 2.9324e-05 - val_beta_loss: 0.0000e+00 - val_loss: 193.1993 - val_rul_loss: 191.3528 - val_rul_mae: 10.6145\n",
            "Epoch 7/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 97ms/step - alpha_loss: 2.6651e-05 - beta_loss: 0.0000e+00 - loss: 175.7684 - rul_loss: 175.7679 - rul_mae: 9.6116 - val_alpha_loss: 3.0927e-05 - val_beta_loss: 0.0000e+00 - val_loss: 193.0871 - val_rul_loss: 191.2097 - val_rul_mae: 10.5772\n",
            "Epoch 8/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 93ms/step - alpha_loss: 2.8392e-05 - beta_loss: 0.0000e+00 - loss: 175.5786 - rul_loss: 175.5779 - rul_mae: 9.6364 - val_alpha_loss: 3.0214e-05 - val_beta_loss: 0.0000e+00 - val_loss: 195.2388 - val_rul_loss: 193.5514 - val_rul_mae: 10.5687\n",
            "Epoch 9/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 97ms/step - alpha_loss: 2.8810e-05 - beta_loss: 0.0000e+00 - loss: 177.1306 - rul_loss: 177.1300 - rul_mae: 9.6640 - val_alpha_loss: 3.2458e-05 - val_beta_loss: 0.0000e+00 - val_loss: 197.0051 - val_rul_loss: 195.0610 - val_rul_mae: 10.9037\n",
            "Epoch 10/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 99ms/step - alpha_loss: 3.0039e-05 - beta_loss: 0.0000e+00 - loss: 169.4195 - rul_loss: 169.4187 - rul_mae: 9.4692 - val_alpha_loss: 3.4314e-05 - val_beta_loss: 0.0000e+00 - val_loss: 198.7783 - val_rul_loss: 196.8635 - val_rul_mae: 10.8889\n",
            "Epoch 11/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 103ms/step - alpha_loss: 3.1866e-05 - beta_loss: 0.0000e+00 - loss: 172.3813 - rul_loss: 172.3809 - rul_mae: 9.5298 - val_alpha_loss: 3.6142e-05 - val_beta_loss: 0.0000e+00 - val_loss: 188.3342 - val_rul_loss: 186.6184 - val_rul_mae: 10.3948\n",
            "Epoch 12/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 99ms/step - alpha_loss: 3.3328e-05 - beta_loss: 0.0000e+00 - loss: 169.2043 - rul_loss: 169.2037 - rul_mae: 9.4410 - val_alpha_loss: 3.6064e-05 - val_beta_loss: 0.0000e+00 - val_loss: 188.2732 - val_rul_loss: 186.5374 - val_rul_mae: 10.4156\n",
            "Epoch 13/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 98ms/step - alpha_loss: 3.3480e-05 - beta_loss: 0.0000e+00 - loss: 164.5903 - rul_loss: 164.5895 - rul_mae: 9.2688 - val_alpha_loss: 3.7907e-05 - val_beta_loss: 0.0000e+00 - val_loss: 192.2725 - val_rul_loss: 190.5612 - val_rul_mae: 10.5488\n",
            "Epoch 14/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 99ms/step - alpha_loss: 3.5394e-05 - beta_loss: 0.0000e+00 - loss: 163.7267 - rul_loss: 163.7261 - rul_mae: 9.2909 - val_alpha_loss: 3.8786e-05 - val_beta_loss: 0.0000e+00 - val_loss: 191.0555 - val_rul_loss: 189.5845 - val_rul_mae: 10.3793\n",
            "Epoch 15/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 94ms/step - alpha_loss: 3.5948e-05 - beta_loss: 0.0000e+00 - loss: 166.2020 - rul_loss: 166.2012 - rul_mae: 9.3401 - val_alpha_loss: 3.9717e-05 - val_beta_loss: 0.0000e+00 - val_loss: 182.7084 - val_rul_loss: 180.9859 - val_rul_mae: 10.2202\n",
            "Epoch 16/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 97ms/step - alpha_loss: 3.8360e-05 - beta_loss: 0.0000e+00 - loss: 160.7911 - rul_loss: 160.7902 - rul_mae: 9.1677 - val_alpha_loss: 3.8486e-05 - val_beta_loss: 0.0000e+00 - val_loss: 181.2898 - val_rul_loss: 179.7167 - val_rul_mae: 9.7719\n",
            "Epoch 17/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 98ms/step - alpha_loss: 3.7567e-05 - beta_loss: 0.0000e+00 - loss: 158.4131 - rul_loss: 158.4126 - rul_mae: 9.0678 - val_alpha_loss: 4.2017e-05 - val_beta_loss: 0.0000e+00 - val_loss: 190.2591 - val_rul_loss: 188.8011 - val_rul_mae: 10.0743\n",
            "Epoch 18/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 93ms/step - alpha_loss: 4.0538e-05 - beta_loss: 0.0000e+00 - loss: 158.8497 - rul_loss: 158.8491 - rul_mae: 9.0518 - val_alpha_loss: 4.0938e-05 - val_beta_loss: 0.0000e+00 - val_loss: 184.3181 - val_rul_loss: 182.9231 - val_rul_mae: 10.0889\n",
            "Epoch 19/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 104ms/step - alpha_loss: 4.0403e-05 - beta_loss: 0.0000e+00 - loss: 153.0944 - rul_loss: 153.0939 - rul_mae: 8.9200 - val_alpha_loss: 4.2998e-05 - val_beta_loss: 0.0000e+00 - val_loss: 187.6922 - val_rul_loss: 185.9287 - val_rul_mae: 10.0126\n",
            "Epoch 20/20\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 99ms/step - alpha_loss: 4.2087e-05 - beta_loss: 0.0000e+00 - loss: 155.8226 - rul_loss: 155.8220 - rul_mae: 8.9842 - val_alpha_loss: 4.2610e-05 - val_beta_loss: 0.0000e+00 - val_loss: 182.1081 - val_rul_loss: 180.5410 - val_rul_mae: 9.5995\n",
            "DualResidual+Prior RMSE: 14.12  NASA: 43773.27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def quick_try(eps, lam, enc_units=192, max_epochs=15):\n",
        "    model = build_dual_sensor_time_residual(input_shape, enc_units=enc_units, eps=eps, drop=0.25)\n",
        "    def alpha_prior_loss(y_true, alpha):\n",
        "        p_batch = tf.constant(p)[None, :]\n",
        "        return lam * tf.reduce_mean(tf.square(alpha - p_batch))\n",
        "    model.compile(optimizers.Adam(6e-4),\n",
        "                  loss={\"rul\":\"mse\",\"alpha\":alpha_prior_loss,\"beta\":lambda yt,yp:0.0*yp},\n",
        "                  loss_weights={\"rul\":1.0,\"alpha\":1.0,\"beta\":0.0},\n",
        "                  metrics={\"rul\":[\"mae\"]})\n",
        "    es = callbacks.EarlyStopping(monitor=\"val_loss\", patience=6, restore_best_weights=True)\n",
        "    model.fit(X_train, {\"rul\": y_train, \"alpha\": np.zeros((X_train.shape[0], Fin)), \"beta\": np.zeros((X_train.shape[0], X_train.shape[1]))},\n",
        "              validation_split=0.2, epochs=max_epochs, batch_size=64, verbose=0, callbacks=[es])\n",
        "    y_pred, a_out, b_out = model.predict(X_test, verbose=0)\n",
        "    rmse = float(np.sqrt(np.mean((y_pred - y_test)**2)))\n",
        "    nasa = nasa_score(y_test, y_pred)\n",
        "\n",
        "    return rmse, nasa, model, a_out\n",
        "\n",
        "grid_eps = [0.20, 0.25, 0.30]\n",
        "grid_lam = [0.0, 0.0025, 0.005]\n",
        "results = []\n",
        "for e in grid_eps:\n",
        "    for l in grid_lam:\n",
        "        rmse, nasa, _, _ = quick_try(e, l, enc_units=192, max_epochs=15)\n",
        "        results.append({\"eps\":e, \"lam\":l, \"rmse\":rmse, \"nasa\":nasa})\n",
        "        print(f\"eps={e}, lam={l} -> RMSE={rmse:.2f}, NASA={nasa:.2f}\")\n",
        "\n",
        "# Pick best NASA subject to RMSE <= (GRU_baseline + 0.5); use your measured baseline (e.g., 13.48)\n",
        "rmse_cap = 13.48 + 0.5\n",
        "best = min([r for r in results if r[\"rmse\"] <= rmse_cap], key=lambda r: r[\"nasa\"]) if any(r[\"rmse\"] <= rmse_cap for r in results) else min(results, key=lambda r: r[\"nasa\"])\n",
        "print(\"Selected:\", best)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJk1oHI4r490",
        "outputId": "27a2c7a2-05aa-4f9c-8974-30487fb0066a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eps=0.2, lam=0.0 -> RMSE=14.55, NASA=44676.45\n",
            "eps=0.2, lam=0.0025 -> RMSE=14.20, NASA=43599.70\n",
            "eps=0.2, lam=0.005 -> RMSE=14.84, NASA=46761.45\n",
            "eps=0.25, lam=0.0 -> RMSE=14.60, NASA=38594.07\n",
            "eps=0.25, lam=0.0025 -> RMSE=14.69, NASA=44679.12\n",
            "eps=0.25, lam=0.005 -> RMSE=14.47, NASA=45247.56\n",
            "eps=0.3, lam=0.0 -> RMSE=14.60, NASA=42582.77\n",
            "eps=0.3, lam=0.0025 -> RMSE=14.45, NASA=47554.98\n",
            "eps=0.3, lam=0.005 -> RMSE=14.48, NASA=49079.76\n",
            "Selected: {'eps': 0.25, 'lam': 0.0, 'rmse': 14.596415519714355, 'nasa': 38594.07421875}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Final DualResidual (sensor→time) with best settings from your grid\n",
        "from tensorflow.keras import layers, models, backend as K, optimizers, callbacks\n",
        "import tensorflow as tf, numpy as np\n",
        "\n",
        "def build_dual_sensor_time_residual(input_shape, enc_units=192, eps=0.25, drop=0.25):\n",
        "    Fin = input_shape[-1]\n",
        "    x_in = layers.Input(shape=input_shape)\n",
        "\n",
        "    # Sensor attention over input channels (per-window)\n",
        "    x_mean = layers.Lambda(lambda x: K.mean(x, axis=1))(x_in)                # (B, Fin)\n",
        "    a = layers.Dense(64, activation=\"tanh\")(x_mean)                           # (B, 64)\n",
        "    alpha = layers.Dense(Fin, activation=\"softmax\", name=\"alpha\")(a)          # (B, Fin)\n",
        "    alpha_e = layers.Lambda(lambda a: K.expand_dims(a, axis=1))(alpha)        # (B, 1, Fin)\n",
        "\n",
        "    # Residual gate: x_w = x_in * (1 + eps*(alpha - 1/Fin))\n",
        "    one_over = 1.0 / Fin\n",
        "    def residual_gate(inputs):\n",
        "        x, aE = inputs\n",
        "        return x * (1.0 + eps * (aE - one_over))\n",
        "    x_w = layers.Lambda(residual_gate)([x_in, alpha_e])\n",
        "\n",
        "    # Encoder + temporal attention\n",
        "    H = layers.LSTM(enc_units, return_sequences=True)(x_w)\n",
        "    H = layers.Dropout(drop)(H)\n",
        "    e_t = layers.Dense(1, activation=\"tanh\")(H)\n",
        "    e_t = layers.Flatten()(e_t)\n",
        "    beta = layers.Activation(\"softmax\", name=\"beta\")(e_t)\n",
        "    beta_e = layers.Lambda(lambda b: K.expand_dims(b, -1))(beta)\n",
        "    H_t = layers.Multiply()([H, beta_e])\n",
        "\n",
        "    context = layers.Lambda(lambda x: K.sum(x, axis=1))(H_t)\n",
        "    x = layers.Dense(64, activation=\"relu\")(context)\n",
        "    y = layers.Dense(1, name=\"rul\")(x)\n",
        "    return models.Model(inputs=x_in, outputs=[y, alpha, beta])\n",
        "\n",
        "input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "dual_final = build_dual_sensor_time_residual(input_shape, enc_units=192, eps=0.25, drop=0.25)\n",
        "\n",
        "dual_final.compile(optimizers.Adam(6e-4),\n",
        "                   loss={\"rul\":\"mse\",\"alpha\":lambda yt,yp:0.0*yp,\"beta\":lambda yt,yp:0.0*yp},\n",
        "                   loss_weights={\"rul\":1.0,\"alpha\":0.0,\"beta\":0.0},\n",
        "                   metrics={\"rul\":[\"mae\"]})\n",
        "\n",
        "es = callbacks.EarlyStopping(monitor=\"val_loss\", patience=8, restore_best_weights=True)\n",
        "dual_final.fit(X_train, {\"rul\": y_train,\n",
        "                         \"alpha\": np.zeros((X_train.shape[0], X_train.shape[2])),\n",
        "                         \"beta\":  np.zeros((X_train.shape[0], X_train.shape[1]))},\n",
        "               validation_split=0.2, epochs=30, batch_size=64, verbose=1, callbacks=[es])\n",
        "\n",
        "y_pred_final, alpha_final, beta_final = dual_final.predict(X_test, verbose=0)\n",
        "rmse_final = float(np.sqrt(np.mean((y_pred_final - y_test)**2)))\n",
        "nasa_final = nasa_score(y_test, y_pred_final)\n",
        "print(\"Final DualResidual RMSE:\", round(rmse_final,2), \" NASA:\", round(nasa_final,2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6X7WYrTydhs",
        "outputId": "98318796-a6b0-4dcd-a9b6-c9d9cecf586b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 98ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 4676.9043 - rul_loss: 4676.8857 - rul_mae: 57.5600 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 1754.1765 - val_rul_loss: 1775.8475 - val_rul_mae: 37.9078\n",
            "Epoch 2/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 91ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 1306.7758 - rul_loss: 1306.7765 - rul_mae: 31.5375 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 713.0001 - val_rul_loss: 708.3810 - val_rul_mae: 21.9100\n",
            "Epoch 3/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 97ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 655.5337 - rul_loss: 655.5397 - rul_mae: 20.6387 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 500.7265 - val_rul_loss: 495.7909 - val_rul_mae: 18.1084\n",
            "Epoch 4/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 91ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 540.9058 - rul_loss: 540.9073 - rul_mae: 18.5073 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 490.7521 - val_rul_loss: 486.0799 - val_rul_mae: 18.4183\n",
            "Epoch 5/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 96ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 416.2701 - rul_loss: 416.2731 - rul_mae: 16.0945 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 328.7014 - val_rul_loss: 325.5119 - val_rul_mae: 14.8416\n",
            "Epoch 6/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 98ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 343.6122 - rul_loss: 343.6134 - rul_mae: 14.4937 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 277.3089 - val_rul_loss: 274.5838 - val_rul_mae: 13.1991\n",
            "Epoch 7/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 93ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 287.1584 - rul_loss: 287.1600 - rul_mae: 12.9946 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 263.3298 - val_rul_loss: 260.7093 - val_rul_mae: 12.5327\n",
            "Epoch 8/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 102ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 267.9157 - rul_loss: 267.9154 - rul_mae: 12.4362 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 244.6420 - val_rul_loss: 242.2042 - val_rul_mae: 12.3254\n",
            "Epoch 9/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 97ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 264.1618 - rul_loss: 264.1611 - rul_mae: 12.2205 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 236.6879 - val_rul_loss: 234.3093 - val_rul_mae: 12.1338\n",
            "Epoch 10/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 98ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 252.2883 - rul_loss: 252.2876 - rul_mae: 11.8828 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 227.4341 - val_rul_loss: 225.1491 - val_rul_mae: 11.8680\n",
            "Epoch 11/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 98ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 239.7056 - rul_loss: 239.7048 - rul_mae: 11.5123 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 219.3196 - val_rul_loss: 217.2133 - val_rul_mae: 11.5357\n",
            "Epoch 12/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 97ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 238.0205 - rul_loss: 238.0194 - rul_mae: 11.4689 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 207.0364 - val_rul_loss: 205.0108 - val_rul_mae: 10.9976\n",
            "Epoch 13/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 102ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 226.5449 - rul_loss: 226.5438 - rul_mae: 11.1229 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 218.2645 - val_rul_loss: 216.4000 - val_rul_mae: 11.6431\n",
            "Epoch 14/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 97ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 222.3354 - rul_loss: 222.3344 - rul_mae: 11.0652 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 205.4774 - val_rul_loss: 203.5258 - val_rul_mae: 11.0081\n",
            "Epoch 15/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 92ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 210.0572 - rul_loss: 210.0568 - rul_mae: 10.7219 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 201.0807 - val_rul_loss: 199.0652 - val_rul_mae: 10.7171\n",
            "Epoch 16/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 97ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 215.9342 - rul_loss: 215.9336 - rul_mae: 10.7885 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 199.8019 - val_rul_loss: 197.8089 - val_rul_mae: 10.8061\n",
            "Epoch 17/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 97ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 206.5951 - rul_loss: 206.5942 - rul_mae: 10.5520 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 196.7001 - val_rul_loss: 194.8307 - val_rul_mae: 10.7119\n",
            "Epoch 18/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 92ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 201.4045 - rul_loss: 201.4042 - rul_mae: 10.4093 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 203.9941 - val_rul_loss: 201.9978 - val_rul_mae: 10.9846\n",
            "Epoch 19/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 103ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 200.6445 - rul_loss: 200.6439 - rul_mae: 10.3791 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 202.5120 - val_rul_loss: 200.5944 - val_rul_mae: 10.8015\n",
            "Epoch 20/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 96ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 196.0111 - rul_loss: 196.0105 - rul_mae: 10.2614 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 206.9943 - val_rul_loss: 205.0170 - val_rul_mae: 10.9922\n",
            "Epoch 21/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 98ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 193.7662 - rul_loss: 193.7658 - rul_mae: 10.1843 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 209.9688 - val_rul_loss: 208.0249 - val_rul_mae: 11.0534\n",
            "Epoch 22/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 97ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 190.9672 - rul_loss: 190.9668 - rul_mae: 10.1139 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 212.4399 - val_rul_loss: 210.3465 - val_rul_mae: 10.9823\n",
            "Epoch 23/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 91ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 187.1607 - rul_loss: 187.1605 - rul_mae: 9.9904 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 211.0421 - val_rul_loss: 209.0171 - val_rul_mae: 11.0968\n",
            "Epoch 24/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 97ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 184.3714 - rul_loss: 184.3708 - rul_mae: 9.9103 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 208.2446 - val_rul_loss: 206.1696 - val_rul_mae: 10.8152\n",
            "Epoch 25/30\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 97ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 181.7728 - rul_loss: 181.7723 - rul_mae: 9.8621 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 205.7572 - val_rul_loss: 203.7625 - val_rul_mae: 10.9138\n",
            "Final DualResidual RMSE: 14.6  NASA: 40177.53\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Rebuild and train the selected configuration exactly like the grid run\n",
        "from tensorflow.keras import layers, models, backend as K, optimizers, callbacks\n",
        "import tensorflow as tf, numpy as np\n",
        "\n",
        "def build_dual_sensor_time_residual(input_shape, enc_units=192, eps=0.25, drop=0.25):\n",
        "    Fin = input_shape[-1]\n",
        "    x_in = layers.Input(shape=input_shape)\n",
        "    x_mean = layers.Lambda(lambda x: K.mean(x, axis=1))(x_in)\n",
        "    a = layers.Dense(64, activation=\"tanh\")(x_mean)\n",
        "    alpha = layers.Dense(Fin, activation=\"softmax\", name=\"alpha\")(a)\n",
        "    alpha_e = layers.Lambda(lambda a: K.expand_dims(a, axis=1))(alpha)\n",
        "    one_over = 1.0 / Fin\n",
        "    def residual_gate(inputs):\n",
        "        x, aE = inputs\n",
        "        return x * (1.0 + eps * (aE - one_over))\n",
        "    x_w = layers.Lambda(residual_gate)([x_in, alpha_e])\n",
        "    H = layers.LSTM(enc_units, return_sequences=True)(x_w)\n",
        "    H = layers.Dropout(0.25)(H)\n",
        "    e_t = layers.Dense(1, activation=\"tanh\")(H)\n",
        "    e_t = layers.Flatten()(e_t)\n",
        "    beta = layers.Activation(\"softmax\", name=\"beta\")(e_t)\n",
        "    beta_e = layers.Lambda(lambda b: K.expand_dims(b, -1))(beta)\n",
        "    H_t = layers.Multiply()([H, beta_e])\n",
        "    context = layers.Lambda(lambda x: K.sum(x, axis=1))(H_t)\n",
        "    x = layers.Dense(64, activation=\"relu\")(context)\n",
        "    y = layers.Dense(1, name=\"rul\")(x)\n",
        "    return models.Model(inputs=x_in, outputs=[y, alpha, beta])\n",
        "\n",
        "input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "dual_sel = build_dual_sensor_time_residual(input_shape, enc_units=192, eps=0.25, drop=0.25)\n",
        "\n",
        "dual_sel.compile(optimizers.Adam(6e-4),\n",
        "                 loss={\"rul\":\"mse\",\"alpha\":lambda yt,yp:0.0*yp,\"beta\":lambda yt,yp:0.0*yp},\n",
        "                 loss_weights={\"rul\":1.0,\"alpha\":0.0,\"beta\":0.0},\n",
        "                 metrics={\"rul\":[\"mae\"]})\n",
        "\n",
        "es = callbacks.EarlyStopping(monitor=\"val_loss\", patience=6, restore_best_weights=True)\n",
        "dual_sel.fit(X_train, {\"rul\": y_train,\n",
        "                       \"alpha\": np.zeros((X_train.shape[0], X_train.shape[2])),\n",
        "                       \"beta\":  np.zeros((X_train.shape[0], X_train.shape[1]))},\n",
        "             validation_split=0.2, epochs=15, batch_size=64, verbose=1, callbacks=[es])\n",
        "\n",
        "y_pred_sel, alpha_sel, beta_sel = dual_sel.predict(X_test, verbose=0)\n",
        "rmse_sel = float(np.sqrt(np.mean((y_pred_sel - y_test)**2)))\n",
        "nasa_sel = nasa_score(y_test, y_pred_sel)\n",
        "print(\"Selected DualResidual RMSE:\", round(rmse_sel,2), \" NASA:\", round(nasa_sel,2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRatFY_F9-iN",
        "outputId": "e52821a2-59c4-47ac-a5b2-45283ab4fb4c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 94ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 4900.8535 - rul_loss: 4900.8345 - rul_mae: 59.0314 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 1793.2247 - val_rul_loss: 1821.6633 - val_rul_mae: 38.1179\n",
            "Epoch 2/15\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 98ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 1665.4800 - rul_loss: 1665.4745 - rul_mae: 36.1245 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 863.3378 - val_rul_loss: 858.3541 - val_rul_mae: 25.7303\n",
            "Epoch 3/15\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 92ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 774.1282 - rul_loss: 774.1332 - rul_mae: 23.2627 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 689.5796 - val_rul_loss: 685.3724 - val_rul_mae: 21.4592\n",
            "Epoch 4/15\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 98ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 642.2155 - rul_loss: 642.2178 - rul_mae: 20.0458 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 614.8198 - val_rul_loss: 609.3229 - val_rul_mae: 21.1910\n",
            "Epoch 5/15\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 103ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 521.5623 - rul_loss: 521.5636 - rul_mae: 17.9285 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 446.3857 - val_rul_loss: 441.9926 - val_rul_mae: 17.2782\n",
            "Epoch 6/15\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 102ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 400.1957 - rul_loss: 400.1960 - rul_mae: 15.6158 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 357.1246 - val_rul_loss: 353.5412 - val_rul_mae: 15.7405\n",
            "Epoch 7/15\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 98ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 320.5349 - rul_loss: 320.5363 - rul_mae: 13.8190 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 287.3168 - val_rul_loss: 284.4536 - val_rul_mae: 13.6869\n",
            "Epoch 8/15\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 98ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 295.7541 - rul_loss: 295.7534 - rul_mae: 13.0549 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 286.4534 - val_rul_loss: 283.6169 - val_rul_mae: 13.5044\n",
            "Epoch 9/15\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 100ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 290.3095 - rul_loss: 290.3096 - rul_mae: 12.8726 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 254.1752 - val_rul_loss: 251.6184 - val_rul_mae: 11.9971\n",
            "Epoch 10/15\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 103ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 251.9423 - rul_loss: 251.9420 - rul_mae: 11.8985 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 243.1478 - val_rul_loss: 240.7567 - val_rul_mae: 11.7848\n",
            "Epoch 11/15\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 103ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 242.4350 - rul_loss: 242.4350 - rul_mae: 11.6097 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 231.4456 - val_rul_loss: 229.1484 - val_rul_mae: 11.5111\n",
            "Epoch 12/15\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 97ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 237.9964 - rul_loss: 237.9958 - rul_mae: 11.4970 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 229.3732 - val_rul_loss: 227.1249 - val_rul_mae: 11.1554\n",
            "Epoch 13/15\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 106ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 226.2173 - rul_loss: 226.2166 - rul_mae: 11.1593 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 225.0446 - val_rul_loss: 222.9648 - val_rul_mae: 11.2107\n",
            "Epoch 14/15\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 100ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 222.5316 - rul_loss: 222.5306 - rul_mae: 11.0255 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 217.4012 - val_rul_loss: 215.2796 - val_rul_mae: 11.0134\n",
            "Epoch 15/15\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 96ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 219.4526 - rul_loss: 219.4517 - rul_mae: 10.9462 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 212.1440 - val_rul_loss: 210.0836 - val_rul_mae: 10.8958\n",
            "Selected DualResidual RMSE: 14.4  NASA: 43509.42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Repeat with the exact same recipe to reduce variance\n",
        "from tensorflow.keras import layers, models, backend as K, optimizers, callbacks\n",
        "import tensorflow as tf, numpy as np, random\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    np.random.seed(seed); random.seed(seed); tf.random.set_seed(seed)\n",
        "\n",
        "def build_dual_sensor_time_residual(input_shape, enc_units=192, eps=0.25, drop=0.25):\n",
        "    Fin = input_shape[-1]\n",
        "    x_in = layers.Input(shape=input_shape)\n",
        "    x_mean = layers.Lambda(lambda x: K.mean(x, axis=1))(x_in)\n",
        "    a = layers.Dense(64, activation=\"tanh\")(x_mean)\n",
        "    alpha = layers.Dense(Fin, activation=\"softmax\", name=\"alpha\")(a)\n",
        "    alpha_e = layers.Lambda(lambda a: K.expand_dims(a, axis=1))(alpha)\n",
        "    one_over = 1.0 / Fin\n",
        "    def residual_gate(inputs):\n",
        "        x, aE = inputs\n",
        "        return x * (1.0 + eps * (aE - one_over))\n",
        "    x_w = layers.Lambda(residual_gate)([x_in, alpha_e])\n",
        "    H = layers.LSTM(enc_units, return_sequences=True)(x_w)\n",
        "    H = layers.Dropout(0.25)(H)\n",
        "    e_t = layers.Dense(1, activation=\"tanh\")(H)\n",
        "    e_t = layers.Flatten()(e_t)\n",
        "    beta = layers.Activation(\"softmax\", name=\"beta\")(e_t)\n",
        "    beta_e = layers.Lambda(lambda b: K.expand_dims(b, -1))(beta)\n",
        "    H_t = layers.Multiply()([H, beta_e])\n",
        "    context = layers.Lambda(lambda x: K.sum(x, axis=1))(H_t)\n",
        "    x = layers.Dense(64, activation=\"relu\")(context)\n",
        "    y = layers.Dense(1, name=\"rul\")(x)\n",
        "    return models.Model(inputs=x_in, outputs=[y, alpha, beta])\n",
        "\n",
        "# Use the same seed you used in the grid for closest reproduction; try one repeat with a different seed if you want a second sample\n",
        "set_seed(42)\n",
        "input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "dual_sel = build_dual_sensor_time_residual(input_shape, enc_units=192, eps=0.25, drop=0.25)\n",
        "\n",
        "dual_sel.compile(optimizers.Adam(6e-4),\n",
        "                 loss={\"rul\":\"mse\",\"alpha\":lambda yt,yp:0.0*yp,\"beta\":lambda yt,yp:0.0*yp},\n",
        "                 loss_weights={\"rul\":1.0,\"alpha\":0.0,\"beta\":0.0},\n",
        "                 metrics={\"rul\":[\"mae\"]})\n",
        "\n",
        "es = callbacks.EarlyStopping(monitor=\"val_loss\", patience=6, restore_best_weights=True)\n",
        "dual_sel.fit(X_train, {\"rul\": y_train,\n",
        "                       \"alpha\": np.zeros((X_train.shape[0], X_train.shape[2])),\n",
        "                       \"beta\":  np.zeros((X_train.shape[0], X_train.shape[1]))},\n",
        "             validation_split=0.2, epochs=15, batch_size=64, verbose=1, callbacks=[es])\n",
        "\n",
        "y_pred_sel, alpha_sel, beta_sel = dual_sel.predict(X_test, verbose=0)\n",
        "rmse_sel = float(np.sqrt(np.mean((y_pred_sel - y_test)**2)))\n",
        "nasa_sel = nasa_score(y_test, y_pred_sel)\n",
        "print(\"Selected DualResidual (repeat) RMSE:\", round(rmse_sel,2), \" NASA:\", round(nasa_sel,2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qjFD4a2D008",
        "outputId": "754bf56b-1321-4b53-994c-00681e33f3fb"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 100ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 4742.2720 - rul_loss: 4742.2544 - rul_mae: 57.9910 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 1796.9734 - val_rul_loss: 1825.1964 - val_rul_mae: 38.1705\n",
            "Epoch 2/15\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 93ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 1715.0953 - rul_loss: 1715.0891 - rul_mae: 36.7128 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 901.2676 - val_rul_loss: 896.0163 - val_rul_mae: 26.1626\n",
            "Epoch 3/15\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 102ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 767.8198 - rul_loss: 767.8222 - rul_mae: 23.1149 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 547.3121 - val_rul_loss: 541.6964 - val_rul_mae: 18.6095\n",
            "Epoch 4/15\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 97ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 479.8560 - rul_loss: 479.8576 - rul_mae: 17.1369 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 409.2307 - val_rul_loss: 405.0791 - val_rul_mae: 16.3660\n",
            "Epoch 5/15\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 95ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 359.3089 - rul_loss: 359.3079 - rul_mae: 14.5505 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 344.8476 - val_rul_loss: 341.3369 - val_rul_mae: 15.0623\n",
            "Epoch 6/15\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 102ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 304.4605 - rul_loss: 304.4605 - rul_mae: 13.2562 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 293.4110 - val_rul_loss: 290.4648 - val_rul_mae: 13.0393\n",
            "Epoch 7/15\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 98ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 276.4746 - rul_loss: 276.4735 - rul_mae: 12.4593 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 271.1524 - val_rul_loss: 268.4713 - val_rul_mae: 12.5608\n",
            "Epoch 8/15\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 98ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 276.2838 - rul_loss: 276.2822 - rul_mae: 12.4046 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 256.2137 - val_rul_loss: 253.6450 - val_rul_mae: 12.3442\n",
            "Epoch 9/15\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 97ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 252.2404 - rul_loss: 252.2394 - rul_mae: 11.8429 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 252.6664 - val_rul_loss: 250.1387 - val_rul_mae: 12.0366\n",
            "Epoch 10/15\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 97ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 246.5622 - rul_loss: 246.5612 - rul_mae: 11.6868 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 241.7921 - val_rul_loss: 239.3339 - val_rul_mae: 12.1115\n",
            "Epoch 11/15\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 92ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 238.4012 - rul_loss: 238.4002 - rul_mae: 11.4784 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 234.0081 - val_rul_loss: 231.6436 - val_rul_mae: 11.8009\n",
            "Epoch 12/15\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 98ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 233.6732 - rul_loss: 233.6721 - rul_mae: 11.3364 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 231.2806 - val_rul_loss: 228.9527 - val_rul_mae: 11.7194\n",
            "Epoch 13/15\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 97ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 224.4475 - rul_loss: 224.4464 - rul_mae: 11.0968 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 227.6875 - val_rul_loss: 225.4365 - val_rul_mae: 11.6386\n",
            "Epoch 14/15\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 98ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 219.7489 - rul_loss: 219.7480 - rul_mae: 11.0266 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 230.3126 - val_rul_loss: 228.0302 - val_rul_mae: 11.8303\n",
            "Epoch 15/15\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 102ms/step - alpha_loss: 0.0000e+00 - beta_loss: 0.0000e+00 - loss: 219.9490 - rul_loss: 219.9480 - rul_mae: 11.0164 - val_alpha_loss: 0.0000e+00 - val_beta_loss: 0.0000e+00 - val_loss: 228.3287 - val_rul_loss: 226.0910 - val_rul_mae: 11.5080\n",
            "Selected DualResidual (repeat) RMSE: 14.58  NASA: 40919.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save final metrics and plots based on your latest y_pred_sel, alpha_sel, rmse_sel, nasa_sel\n",
        "import pandas as pd, matplotlib.pyplot as plt, numpy as np\n",
        "\n",
        "# Baselines (replace if you recomputed)\n",
        "rmse_lstm, nasa_lstm = 13.96, 36866.66\n",
        "rmse_gru,  nasa_gru  = 13.48, 35966.99\n",
        "\n",
        "pd.DataFrame([\n",
        "    {\"model\":\"LSTM\",         \"rmse\": rmse_lstm, \"nasa\": nasa_lstm},\n",
        "    {\"model\":\"GRU\",          \"rmse\": rmse_gru,  \"nasa\": nasa_gru},\n",
        "    {\"model\":\"DualResidual\", \"rmse\": rmse_sel,  \"nasa\": nasa_sel},\n",
        "]).to_csv(\"fd001_metrics_final.csv\", index=False)\n",
        "\n",
        "# Prediction vs truth plot (first 200 points)\n",
        "plt.figure(figsize=(6,3))\n",
        "plt.plot(y_test[:200], label=\"true\")\n",
        "plt.plot(y_pred_sel[:200], label=\"dual\")\n",
        "plt.legend(); plt.tight_layout(); plt.savefig(\"fd001_pred_dual_final.png\"); plt.close()\n",
        "\n",
        "# Mean sensor attention plot\n",
        "alpha_mean = alpha_sel.mean(axis=0)\n",
        "plt.figure(figsize=(6,3))\n",
        "plt.bar(range(len(alpha_mean)), alpha_mean)\n",
        "plt.title(\"Sensor attention (mean) – final\")\n",
        "plt.tight_layout(); plt.savefig(\"fd001_alpha_mean_final.png\"); plt.close()\n",
        "\n",
        "print(\"Saved fd001_metrics_final.csv, fd001_pred_dual_final.png, fd001_alpha_mean_final.png\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzidFeA7Hw8z",
        "outputId": "29da5061-42bc-4799-db9d-578efc1171df"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved fd001_metrics_final.csv, fd001_pred_dual_final.png, fd001_alpha_mean_final.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# One additional seed to measure interpretability stability\n",
        "import tensorflow as tf, numpy as np, random\n",
        "from scipy.stats import spearmanr\n",
        "from tensorflow.keras import optimizers, callbacks\n",
        "\n",
        "def set_seed(seed=123):\n",
        "    np.random.seed(seed); random.seed(seed); tf.random.set_seed(seed)\n",
        "\n",
        "alpha_mean_a = alpha_sel.mean(axis=0)\n",
        "\n",
        "set_seed(123)\n",
        "dual_sel_b = build_dual_sensor_time_residual((X_train.shape[1], X_train.shape[2]), enc_units=192, eps=0.25, drop=0.25)\n",
        "dual_sel_b.compile(optimizers.Adam(6e-4),\n",
        "                   loss={\"rul\":\"mse\",\"alpha\":lambda yt,yp:0.0*yp,\"beta\":lambda yt,yp:0.0*yp},\n",
        "                   loss_weights={\"rul\":1.0,\"alpha\":0.0,\"beta\":0.0},\n",
        "                   metrics={\"rul\":[\"mae\"]})\n",
        "es2 = callbacks.EarlyStopping(monitor=\"val_loss\", patience=6, restore_best_weights=True)\n",
        "dual_sel_b.fit(X_train, {\"rul\": y_train,\n",
        "                         \"alpha\": np.zeros((X_train.shape[0], X_train.shape[2])),\n",
        "                         \"beta\":  np.zeros((X_train.shape[0], X_train.shape[1]))},\n",
        "               validation_split=0.2, epochs=15, batch_size=64, verbose=0, callbacks=[es2])\n",
        "\n",
        "_, alpha_b, _ = dual_sel_b.predict(X_test, verbose=0)\n",
        "alpha_mean_b = alpha_b.mean(axis=0)\n",
        "rho, pval = spearmanr(alpha_mean_a, alpha_mean_b)\n",
        "print(f\"Sensor-attention stability (Spearman): rho={rho:.3f}, p={pval:.3g}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2EUynu_H-sP",
        "outputId": "362930dc-f6d0-4989-cda6-c4f3cb8064ad"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sensor-attention stability (Spearman): rho=0.413, p=0.0448\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create fd001_run_summary.json if missing\n",
        "import json, time, os\n",
        "\n",
        "summary = {\n",
        "  \"dataset\": \"FD001\",\n",
        "  \"window\": 30,\n",
        "  \"model\": \"DualResidual(sensor->time)\",\n",
        "  \"eps\": 0.25,\n",
        "  \"lambda_prior\": 0.0,\n",
        "  \"enc_units\": 192,\n",
        "  \"optimizer\": \"Adam(6e-4)\",\n",
        "  \"batch_size\": 64,\n",
        "  \"patience\": 6,\n",
        "  \"rmse\": float(rmse_sel),\n",
        "  \"nasa\": float(nasa_sel),\n",
        "  \"stability_spearman_rho\": float(rho) if 'rho' in globals() else None,\n",
        "  \"stability_spearman_p\": float(pval) if 'pval' in globals() else None,\n",
        "  \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "}\n",
        "with open(\"fd001_run_summary.json\",\"w\") as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "print(\"Saved fd001_run_summary.json\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpkpjMhkKSlB",
        "outputId": "26018e7e-2c8c-462b-c82d-dc50915ded4d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved fd001_run_summary.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, zipfile, glob\n",
        "\n",
        "artifacts = [\n",
        "  \"fd001_metrics_final.csv\",\n",
        "  \"fd001_pred_dual_final.png\",\n",
        "  \"fd001_alpha_mean_final.png\",\n",
        "  \"fd001_alpha_mean_ensemble.png\",\n",
        "  \"fd001_alpha_mean_ensemble.csv\",\n",
        "  \"fd001_run_summary.json\"\n",
        "]\n",
        "artifacts = [a for a in artifacts if os.path.exists(a)]\n",
        "with zipfile.ZipFile(\"FD001_package.zip\", \"w\", zipfile.ZIP_DEFLATED) as z:\n",
        "    for a in artifacts:\n",
        "        z.write(a)\n",
        "        print(\"Added:\", a)\n",
        "print(\"Wrote FD001_package.zip\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZv_FbJWOm1l",
        "outputId": "f7b30ea4-de11-4294-a87d-a26371d938cf"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added: fd001_metrics_final.csv\n",
            "Added: fd001_pred_dual_final.png\n",
            "Added: fd001_alpha_mean_final.png\n",
            "Added: fd001_run_summary.json\n",
            "Wrote FD001_package.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If not already created, ensemble attention across three seeds for a stable plot\n",
        "import numpy as np, random, tensorflow as tf\n",
        "from tensorflow.keras import optimizers, callbacks\n",
        "\n",
        "def train_and_alpha(seed):\n",
        "    np.random.seed(seed); random.seed(seed); tf.random.set_seed(seed)\n",
        "    m = build_dual_sensor_time_residual((X_train.shape[1], X_train.shape[2]), enc_units=192, eps=0.25, drop=0.25)\n",
        "    m.compile(optimizers.Adam(6e-4),\n",
        "              loss={\"rul\":\"mse\",\"alpha\":lambda yt,yp:0.0*yp,\"beta\":lambda yt,yp:0.0*yp},\n",
        "              loss_weights={\"rul\":1.0,\"alpha\":0.0,\"beta\":0.0},\n",
        "              metrics={\"rul\":[\"mae\"]})\n",
        "    es = callbacks.EarlyStopping(monitor=\"val_loss\", patience=6, restore_best_weights=True)\n",
        "    m.fit(X_train, {\"rul\": y_train,\n",
        "                    \"alpha\": np.zeros((X_train.shape[0], X_train.shape[2])),\n",
        "                    \"beta\":  np.zeros((X_train.shape[0], X_train.shape[1]))},\n",
        "          validation_split=0.2, epochs=15, batch_size=64, verbose=0, callbacks=[es])\n",
        "    _, a, _ = m.predict(X_test, verbose=0)\n",
        "    return a.mean(axis=0)\n",
        "\n",
        "seeds = [42, 123, 2025]\n",
        "alphas = [train_and_alpha(s) for s in seeds]\n",
        "alpha_ens = np.mean(alphas, axis=0)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "np.savetxt(\"fd001_alpha_mean_ensemble.csv\", alpha_ens, delimiter=\",\")\n",
        "plt.figure(figsize=(6,3)); plt.bar(range(len(alpha_ens)), alpha_ens)\n",
        "plt.title(\"Sensor attention (mean) – 3-seed ensemble\")\n",
        "plt.tight_layout(); plt.savefig(\"fd001_alpha_mean_ensemble.png\"); plt.close()\n",
        "print(\"Saved fd001_alpha_mean_ensemble.csv and fd001_alpha_mean_ensemble.png\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skEXLG2_Or_R",
        "outputId": "c47c4b26-ae17-4d7c-9f63-ef888425996e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved fd001_alpha_mean_ensemble.csv and fd001_alpha_mean_ensemble.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "md = f\"\"\"\n",
        "# FD001 RUL Prediction — Methods and Results\n",
        "\n",
        "## Methods\n",
        "- Dataset: CMAPSS FD001 with 100 train/test engines; 26 columns (engine, cycle, 3 operating settings, 21 sensors). Parsed via whitespace with standardized names. [Protocol-aligned]\n",
        "- Preprocessing: Train-only MinMax scaling; 30-step sliding windows per engine; train labels use capped run-to-failure RUL; test labels reconstructed with RUL_FD001 as per PHM protocol. [Deterministic]\n",
        "- Models: LSTM and GRU baselines; DualResidual attention (sensor→time): residual sensor gating x_w = x_in · (1 + eps · (alpha − 1/Fin)) followed by temporal softmax attention over encoder states.\n",
        "- Selection: Small grid over eps ∈ {{0.20,0.25,0.30}} and lambda ∈ {{0,0.0025,0.005}}; choose lowest NASA subject to RMSE ≤ baseline+0.5.\n",
        "- Metrics: RMSE and NASA asymmetric score (late errors penalized more).\n",
        "\n",
        "## Results (FD001)\n",
        "- LSTM: RMSE = {13.96:.2f}, NASA = {36866.66:.0f}\n",
        "- GRU: RMSE = {13.48:.2f}, NASA = {35966.99:.0f}\n",
        "- DualResidual (eps=0.25, lambda=0): RMSE = {rmse_sel:.2f}, NASA = {nasa_sel:.0f}\n",
        "- Interpretability: Spearman rho between mean sensor-attention for two seeds ≈ {rho:.2f} (p ≈ {pval:.3f}); optional 3-seed ensemble attention vector exported.\n",
        "\n",
        "## Artifacts\n",
        "- fd001_metrics_final.csv\n",
        "- fd001_pred_dual_final.png\n",
        "- fd001_alpha_mean_final.png\n",
        "- fd001_alpha_mean_ensemble.png (optional)\n",
        "- fd001_run_summary.json\n",
        "\n",
        "## Notes\n",
        "- Physics prior (lambda > 0) did not improve NASA under this setup; reported as an ablation only.\n",
        "- Training used early stopping with patience=6 and Adam(6e-4); identical preprocessing/splits across all models ensure fair comparison.\n",
        "\"\"\"\n",
        "with open(\"FD001_methods_results.md\",\"w\") as f:\n",
        "    f.write(md)\n",
        "print(\"Wrote FD001_methods_results.md\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQNvPOAMTGO7",
        "outputId": "a3216626-6cc5-4d86-81e8-31508fb1f880"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote FD001_methods_results.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mask one sensor at test time to gauge robustness (e.g., zero out s2 across test sequences)\n",
        "import numpy as np\n",
        "sensor_names = [c for c in train.columns if c not in [\"engine\",\"cycle\"]]\n",
        "s_idx = sensor_names.index(\"s2\")\n",
        "X_test_mask = X_test.copy()\n",
        "X_test_mask[:,:,s_idx] = 0.0\n",
        "y_pred_mask = dual_sel.predict(X_test_mask, verbose=0)[0]\n",
        "rmse_mask = float(np.sqrt(np.mean((y_pred_mask - y_test)**2)))\n",
        "nasa_mask = nasa_score(y_test, y_pred_mask)\n",
        "print(\"Test-time s2 masked -> RMSE:\", round(rmse_mask,2), \" NASA:\", round(nasa_mask,2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZeTxLdKZTUMo",
        "outputId": "01625324-08e5-4919-fa43-3677eaff6591"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test-time s2 masked -> RMSE: 18.99  NASA: 143324.17\n"
          ]
        }
      ]
    }
  ]
}